{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2645886,"sourceType":"datasetVersion","datasetId":1608934},{"sourceId":12745533,"sourceType":"datasetVersion","datasetId":672377}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ‚úÖ Step 1: Imports and Setup\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Input, GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import MobileNetV2, VGG16, EfficientNetB0\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\nfrom tensorflow.keras.applications.vgg16 import preprocess_input as vgg16_preprocess\nfrom tensorflow.keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nimport tensorflow as tf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T16:25:29.222228Z","iopub.execute_input":"2025-10-09T16:25:29.222386Z","iopub.status.idle":"2025-10-09T16:25:43.416846Z","shell.execute_reply.started":"2025-10-09T16:25:29.222371Z","shell.execute_reply":"2025-10-09T16:25:43.416183Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ‚úÖ Step 2: Load and Preprocess Dataset\ndef load_data_from_nested_dirs(data_dir):\n    images = []\n    labels = []\n    for split in ['Training', 'Testing']:\n        split_dir = os.path.join(data_dir, split)\n        if not os.path.exists(split_dir):\n            continue\n        for class_name in os.listdir(split_dir):\n            class_path = os.path.join(split_dir, class_name)\n            if not os.path.isdir(class_path):\n                continue\n            for img_file in os.listdir(class_path):\n                if img_file.endswith(('.jpg', '.jpeg', '.png')):\n                    img_path = os.path.join(class_path, img_file)\n                    label = class_name.lower().replace('no_tumor', 'notumor').replace('_tumor', '')\n                    images.append(img_path)\n                    labels.append(label)\n    return images, labels\n\nbase_dir = \"/kaggle/input\"\ndataset1 = os.path.join(base_dir, \"brain-tumor-mri-dataset\")\ndataset2 = os.path.join(base_dir, \"brain-tumor-classification-mri\")\n\nimages1, labels1 = load_data_from_nested_dirs(dataset1)\nimages2, labels2 = load_data_from_nested_dirs(dataset2)\n\nimages = images1 + images2\nlabels = labels1 + labels2\n\nle = LabelEncoder()\nlabels_encoded = le.fit_transform(labels)\n\nX_train, X_val, y_train, y_val = train_test_split(images, labels_encoded, test_size=0.2, stratify=labels_encoded, random_state=42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T16:25:43.418790Z","iopub.execute_input":"2025-10-09T16:25:43.419249Z","iopub.status.idle":"2025-10-09T16:25:43.635481Z","shell.execute_reply.started":"2025-10-09T16:25:43.419230Z","shell.execute_reply":"2025-10-09T16:25:43.634937Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ‚úÖ Step 3: Resize and Normalize\nIMG_SIZE = (224, 224)\n\ndef load_and_resize(img_paths):\n    return np.array([cv2.resize(cv2.imread(path), IMG_SIZE) for path in img_paths])\n\nX_train_resized = load_and_resize(X_train)\nX_val_resized = load_and_resize(X_val)\n\ny_train_cat = to_categorical(y_train, num_classes=4)\ny_val_cat = to_categorical(y_val, num_classes=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T16:25:43.636092Z","iopub.execute_input":"2025-10-09T16:25:43.636299Z","iopub.status.idle":"2025-10-09T16:27:11.147519Z","shell.execute_reply.started":"2025-10-09T16:25:43.636283Z","shell.execute_reply":"2025-10-09T16:27:11.146861Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Define dataset paths\ndataset1_path = \"/kaggle/input/brain-tumor-mri-dataset\"\ndataset2_path = \"/kaggle/input/brain-tumor-classification-mri\"\n\n# Class name mapping for standardization\nname_map = {\n    'glioma_tumor': 'glioma',\n    'meningioma_tumor': 'meningioma',\n    'pituitary_tumor': 'pituitary',\n    'no_tumor': 'notumor',\n    'glioma': 'glioma',\n    'meningioma': 'meningioma',\n    'pituitary': 'pituitary',\n    'notumor': 'notumor'\n}\n\n# Function to collect metadata from a dataset\ndef collect_dataset_info(base_path, dataset_name):\n    data = []\n\n    for split in ['Training', 'Testing']:\n        split_path = os.path.join(base_path, split)\n        if not os.path.exists(split_path):\n            continue\n\n        for class_folder in os.listdir(split_path):\n            class_path = os.path.join(split_path, class_folder)\n            if not os.path.isdir(class_path):\n                continue\n\n            class_name = name_map.get(class_folder, class_folder)\n\n            for img_file in os.listdir(class_path):\n                if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n                    img_path = os.path.join(class_path, img_file)\n                    try:\n                        with Image.open(img_path) as img:\n                            width, height = img.size\n                            mode = img.mode\n                        data.append({\n                            'dataset': dataset_name,\n                            'split': split,\n                            'class': class_name,\n                            'image_name': img_file,\n                            'path': img_path,\n                            'width': width,\n                            'height': height,\n                            'mode': mode\n                        })\n                    except Exception as e:\n                        print(f\"Error reading {img_path}: {e}\")\n    return pd.DataFrame(data)\n\n# Load metadata from both datasets\ndf1 = collect_dataset_info(dataset1_path, \"dataset1\")\ndf2 = collect_dataset_info(dataset2_path, \"dataset2\")\n\n# Merge them\ndf = pd.concat([df1, df2], ignore_index=True)\n\n# ---- EDA Starts ----\n\nprint(\"Total images in merged dataset:\", len(df))\nprint(df.head())\n\n# üìä Class distribution by split\nplt.figure(figsize=(10, 5))\nsns.countplot(data=df, x='class', hue='split', palette='Set2')\nplt.title(\"Image Distribution by Class and Split\")\nplt.xticks(rotation=15)\nplt.tight_layout()\nplt.show()\n\n# üìè Image size distributions\nplt.figure(figsize=(8, 5))\nsns.histplot(df['width'], bins=30, color='skyblue', label='Width', kde=True)\nsns.histplot(df['height'], bins=30, color='orange', label='Height', kde=True)\nplt.legend()\nplt.title(\"Image Dimension Distribution\")\nplt.tight_layout()\nplt.show()\n\n# üñºÔ∏è Show example images\ndef show_samples(df, split='Training'):\n    classes = df['class'].unique()\n    plt.figure(figsize=(15, 8))\n    for i, cls in enumerate(classes):\n        sample = df[(df['class'] == cls) & (df['split'] == split)].iloc[0]\n        img = Image.open(sample['path'])\n        plt.subplot(2, len(classes)//2 + 1, i+1)\n        plt.imshow(img)\n        plt.title(f\"{split}: {cls}\")\n        plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n# Show sample images\nshow_samples(df, split='Training')\nshow_samples(df, split='Testing')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T16:27:11.148435Z","iopub.execute_input":"2025-10-09T16:27:11.148669Z","iopub.status.idle":"2025-10-09T16:27:22.943078Z","shell.execute_reply.started":"2025-10-09T16:27:11.148649Z","shell.execute_reply":"2025-10-09T16:27:22.942313Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# First, make a copy of the merged dataset\nbalanced_2500_df = pd.DataFrame()\n\ntarget_count = 2500\n\nfor cls in df['class'].unique():\n    class_df = df[df['class'] == cls]\n    \n    # Downsample to 2500 if necessary\n    sampled_df = class_df.sample(n=target_count, random_state=42)\n    \n    balanced_2500_df = pd.concat([balanced_2500_df, sampled_df], ignore_index=True)\n\n# Shuffle the final DataFrame\nbalanced_2500_df = balanced_2500_df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# Confirm the result\nprint(\"‚úÖ Class-wise Count After Forcing All to 2500:\")\nprint(balanced_2500_df['class'].value_counts())\n\n\nplt.figure(figsize=(8, 5))\nsns.countplot(data=balanced_2500_df, x='class', palette='pastel')\nplt.title(\"Balanced Dataset with 2500 Images per Class\")\nplt.xlabel(\"Tumor Class\")\nplt.ylabel(\"Image Count\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T16:27:22.943899Z","iopub.execute_input":"2025-10-09T16:27:22.944160Z","iopub.status.idle":"2025-10-09T16:27:23.101146Z","shell.execute_reply.started":"2025-10-09T16:27:22.944141Z","shell.execute_reply":"2025-10-09T16:27:23.100420Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\n# Split 2500 images per class into train/test (80/20)\nsplit_balanced = []\n\n# Define the desired class order\nclass_order = ['glioma', 'meningioma', 'pituitary', 'notumor']\n\n# Ensure consistent lowercase class names\nbalanced_2500_df['class'] = balanced_2500_df['class'].str.lower()\n\nfor cls in class_order:\n    class_df = balanced_2500_df[balanced_2500_df['class'] == cls]\n    \n    train, test = train_test_split(class_df, test_size=0.2, random_state=42, shuffle=True)\n    \n    train = train.copy()\n    test = test.copy()\n    train['split'] = 'Training'\n    test['split'] = 'Testing'\n    \n    split_balanced.extend([train, test])\n\n# Merge all splits\nsplit_df = pd.concat(split_balanced, ignore_index=True)\n\n# Convert 'class' column to ordered categorical to control plot order\nsplit_df['class'] = pd.Categorical(split_df['class'], categories=class_order, ordered=True)\n\n# Plot\nplt.figure(figsize=(10, 6))\nsns.countplot(data=split_df, x='class', hue='split', palette='Set2')\n\nplt.title(\"Balanced Dataset - Train/Test Split\")\nplt.xlabel(\"Tumor Class\")\nplt.ylabel(\"Number of Images\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T16:27:23.101891Z","iopub.execute_input":"2025-10-09T16:27:23.102117Z","iopub.status.idle":"2025-10-09T16:27:23.314712Z","shell.execute_reply.started":"2025-10-09T16:27:23.102093Z","shell.execute_reply":"2025-10-09T16:27:23.313994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ‚úÖ Step 4: Compute Class Weights\nclass_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\nclass_weights_dict = dict(enumerate(class_weights))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T16:27:23.317077Z","iopub.execute_input":"2025-10-09T16:27:23.317288Z","iopub.status.idle":"2025-10-09T16:27:23.322671Z","shell.execute_reply.started":"2025-10-09T16:27:23.317272Z","shell.execute_reply":"2025-10-09T16:27:23.322020Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_custom_cnn():\n    inputs = Input(shape=(224, 224, 3))\n    x = Conv2D(32, (3, 3), activation='relu')(inputs)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D()(x)\n\n    x = Conv2D(64, (3, 3), activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D()(x)\n\n    x = Conv2D(128, (3, 3), activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D()(x)\n\n    x = Dropout(0.4)(x)\n    x = Flatten()(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    outputs = Dense(4, activation='softmax')(x)\n\n    model = Model(inputs=inputs, outputs=outputs)\n    return model\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T16:27:23.323240Z","iopub.execute_input":"2025-10-09T16:27:23.323418Z","iopub.status.idle":"2025-10-09T16:27:23.337098Z","shell.execute_reply.started":"2025-10-09T16:27:23.323396Z","shell.execute_reply":"2025-10-09T16:27:23.336489Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ‚úÖ Step 6: Build Transfer Learning Wrapper\ndef build_transfer_model(base, preprocess):\n    base_model = base(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n    base_model.trainable = False\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    predictions = Dense(4, activation='softmax')(x)\n    model = Model(inputs=base_model.input, outputs=predictions)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T16:27:23.337736Z","iopub.execute_input":"2025-10-09T16:27:23.337952Z","iopub.status.idle":"2025-10-09T16:27:23.352678Z","shell.execute_reply.started":"2025-10-09T16:27:23.337937Z","shell.execute_reply":"2025-10-09T16:27:23.352143Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ‚úÖ Step 7: Train All Models\nmodels = {\n    \"CustomCNN\": build_custom_cnn(),\n    \"VGG16\": build_transfer_model(VGG16, vgg16_preprocess),\n    \"MobileNetV2\": build_transfer_model(MobileNetV2, mobilenet_preprocess),\n    \"EfficientNetB0\": build_transfer_model(EfficientNetB0, efficientnet_preprocess)\n}\n\nhistories = {}\nmodel_preds = {}\nmodel_scores = {}\n\nfor name, model in models.items():\n    print(f\"\\nTraining {name}...\")\n    model.compile(optimizer=Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n\n    if name == \"CustomCNN\":\n        X_train_input = X_train_resized / 255.\n        X_val_input = X_val_resized / 255.\n    elif name == \"VGG16\":\n        X_train_input = vgg16_preprocess(X_train_resized.copy())\n        X_val_input = vgg16_preprocess(X_val_resized.copy())\n    elif name == \"MobileNetV2\":\n        X_train_input = mobilenet_preprocess(X_train_resized.copy())\n        X_val_input = mobilenet_preprocess(X_val_resized.copy())\n    elif name == \"EfficientNetB0\":\n        X_train_input = efficientnet_preprocess(X_train_resized.copy())\n        X_val_input = efficientnet_preprocess(X_val_resized.copy())\n\n    history = model.fit(\n        X_train_input, y_train_cat,\n        validation_data=(X_val_input, y_val_cat),\n        epochs=50,\n        batch_size=32,\n        class_weight=class_weights_dict,\n        callbacks=[EarlyStopping(patience=9, restore_best_weights=True), ReduceLROnPlateau(patience=5)],\n        verbose=2\n    )\n    histories[name] = history\n\n    y_pred = model.predict(X_val_input)\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    acc = np.mean(y_pred_classes == y_val)\n    model_preds[name] = y_pred\n    model_scores[name] = acc\n\n    print(f\"\\n{name} Accuracy: {acc * 100:.2f}%\")\n    print(classification_report(y_val, y_pred_classes, target_names=le.classes_))\n    cm = confusion_matrix(y_val, y_pred_classes)\n    ConfusionMatrixDisplay(cm, display_labels=le.classes_).plot(cmap='Blues')\n    plt.title(f\"{name} Confusion Matrix\")\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T16:27:23.353274Z","iopub.execute_input":"2025-10-09T16:27:23.353437Z","iopub.status.idle":"2025-10-09T17:07:06.666741Z","shell.execute_reply.started":"2025-10-09T16:27:23.353425Z","shell.execute_reply":"2025-10-09T17:07:06.665918Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ‚úÖ Step 8: Ensemble Prediction (Soft Voting)\nall_preds = np.mean(list(model_preds.values()), axis=0)\ny_ensemble = np.argmax(all_preds, axis=1)\nensemble_acc = np.mean(y_ensemble == y_val)\nprint(f\"\\nüìå Ensemble Accuracy: {ensemble_acc * 100:.2f}%\")\nprint(classification_report(y_val, y_ensemble, target_names=le.classes_))\ncm_ensemble = confusion_matrix(y_val, y_ensemble)\nConfusionMatrixDisplay(cm_ensemble, display_labels=le.classes_).plot(cmap='Blues')\nplt.title(\"üß† Ensemble Confusion Matrix\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T17:07:06.667619Z","iopub.execute_input":"2025-10-09T17:07:06.668215Z","iopub.status.idle":"2025-10-09T17:07:06.859155Z","shell.execute_reply.started":"2025-10-09T17:07:06.668193Z","shell.execute_reply":"2025-10-09T17:07:06.858462Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ‚úÖ Step 9: Grad-CAM Visualization for All Models (Kaggle-compatible)\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import Model\nimport tensorflow as tf\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Patch CustomCNN so it's 'called'\n_ = models[\"CustomCNN\"].predict(np.expand_dims(X_val_resized[0] / 255., axis=0))\n\n\n# ‚úÖ Grad-CAM Generator Function (Robust for all models)\ndef generate_grad_cam(model, image, label_index, preprocess=None, model_name=\"Model\"):\n    # Preprocess image\n    img_tensor = np.expand_dims(image, axis=0)\n    if preprocess:\n        img_tensor = preprocess(img_tensor.copy())\n    else:\n        img_tensor = img_tensor / 255.0\n\n    # üõ† Build/Call Sequential models manually to fix the \"never been called\" error\n    try:\n        _ = model.predict(img_tensor)\n    except:\n        model.build((None, 224, 224, 3))\n        _ = model.predict(img_tensor)\n\n    # üîç Find last Conv2D layer\n    last_conv_layer = None\n    for layer in reversed(model.layers):\n        if isinstance(layer, tf.keras.layers.Conv2D):\n            last_conv_layer = layer.name\n            break\n    if not last_conv_layer:\n        raise ValueError(f\"No Conv2D layer found in model {model_name}\")\n\n    # üéØ Build Grad-CAM model\n    grad_model = Model(\n        inputs=model.input,\n        outputs=[model.get_layer(last_conv_layer).output, model.output]\n    )\n\n    # üî• Compute gradients\n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(img_tensor)\n        loss = predictions[:, label_index]\n\n    grads = tape.gradient(loss, conv_outputs)[0]\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1))\n    conv_outputs = conv_outputs[0]\n\n    heatmap = tf.reduce_mean(tf.multiply(pooled_grads, conv_outputs), axis=-1)\n    heatmap = np.maximum(heatmap, 0) / tf.math.reduce_max(heatmap + tf.keras.backend.epsilon())\n    heatmap = cv2.resize(heatmap.numpy(), (224, 224))\n    heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n\n    superimposed_img = cv2.addWeighted(image.astype(np.uint8), 0.6, heatmap_colored, 0.4, 0)\n    return superimposed_img\n\n# ‚úÖ Preprocessing for each model\npreprocessors = {\n    \"CustomCNN\": None,\n    \"VGG16\": vgg16_preprocess,\n    \"MobileNetV2\": mobilenet_preprocess,\n    \"EfficientNetB0\": efficientnet_preprocess,\n    \n}\n\n# ‚úÖ Sample image from validation set\nsample_img = X_val_resized[0]\ntrue_label = y_val[0]\n\n# üî• Generate Grad-CAM visualizations\nplt.figure(figsize=(20, 5))\nfor i, (name, model) in enumerate(models.items()):\n    print(f\"üîç Generating Grad-CAM for {name}...\")\n    try:\n        cam_img = generate_grad_cam(model, sample_img, true_label, preprocess=preprocessors[name], model_name=name)\n        plt.subplot(1, 4, i + 1)\n        plt.imshow(cam_img)\n        plt.title(f\"{name} Grad-CAM\")\n        plt.axis('off')\n    except Exception as e:\n        print(f\"‚ùå Failed for {name}: {e}\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T17:07:06.859905Z","iopub.execute_input":"2025-10-09T17:07:06.860164Z","iopub.status.idle":"2025-10-09T17:07:18.078507Z","shell.execute_reply.started":"2025-10-09T17:07:06.860146Z","shell.execute_reply":"2025-10-09T17:07:18.077536Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ‚úÖ Step 10: Save Best Model\nbest_model_name = max(model_scores, key=model_scores.get)\nprint(f\"\\n‚úÖ Best Performing Model: {best_model_name} with Accuracy: {model_scores[best_model_name] * 100:.2f}%\")\nmodels[best_model_name].save(f\"best_model_{best_model_name}.h5\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T17:07:18.079517Z","iopub.execute_input":"2025-10-09T17:07:18.079847Z","iopub.status.idle":"2025-10-09T17:07:18.377014Z","shell.execute_reply.started":"2025-10-09T17:07:18.079820Z","shell.execute_reply":"2025-10-09T17:07:18.376445Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ‚úÖ Step 11: Display All Accuracies\nprint(\"\\nüìä All Model Accuracies:\")\nfor model_name, score in model_scores.items():\n    print(f\"{model_name}: {score * 100:.2f}%\")\nprint(f\"Ensemble: {ensemble_acc * 100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T17:07:18.377825Z","iopub.execute_input":"2025-10-09T17:07:18.378090Z","iopub.status.idle":"2025-10-09T17:07:18.382818Z","shell.execute_reply.started":"2025-10-09T17:07:18.378065Z","shell.execute_reply":"2025-10-09T17:07:18.382091Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\nfrom tensorflow.keras.applications.vgg16 import preprocess_input as vgg16_preprocess\nfrom tensorflow.keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\n\n# Grad-CAM Function\ndef generate_grad_cam(model, image, label_index, preprocess=None):\n    img_tensor = np.expand_dims(image, axis=0)\n    if preprocess:\n        img_tensor = preprocess(img_tensor.copy())\n    else:\n        img_tensor = img_tensor / 255.0\n\n    # Detect last Conv layer\n    last_conv_layer = None\n    for layer in reversed(model.layers):\n        if isinstance(layer, tf.keras.layers.Conv2D):\n            last_conv_layer = layer.name\n            break\n\n    grad_model = Model([model.inputs], [model.get_layer(last_conv_layer).output, model.output])\n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(img_tensor)\n        loss = predictions[:, label_index]\n\n    grads = tape.gradient(loss, conv_outputs)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n    conv_outputs = conv_outputs[0]\n    heatmap = tf.reduce_mean(tf.multiply(pooled_grads, conv_outputs), axis=-1)\n    heatmap = np.maximum(heatmap, 0) / tf.reduce_max(heatmap)\n    heatmap = cv2.resize(heatmap.numpy(), (224, 224))\n    heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n    superimposed_img = cv2.addWeighted(image.astype(np.uint8), 0.6, heatmap_colored, 0.4, 0)\n    return superimposed_img\n\n# üîç Sample image (1st from validation set)\nsample_img = X_val_resized[0]\ntrue_label = y_val[0]\n\n# Define preprocessors\npreprocessors = {\n    \"CustomCNN\": None,\n    \"VGG16\": vgg16_preprocess,\n    \"MobileNetV2\": mobilenet_preprocess,\n    \"EfficientNetB0\": efficientnet_preprocess\n}\n\n# üî• Plot Grad-CAMs\nplt.figure(figsize=(16, 8))\nfor i, (name, model) in enumerate(models.items()):\n    grad_cam_img = generate_grad_cam(model, sample_img, true_label, preprocessors[name])\n    plt.subplot(1, 4, i+1)\n    plt.imshow(grad_cam_img)\n    plt.title(f\"{name} Grad-CAM\")\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T17:07:18.383978Z","iopub.execute_input":"2025-10-09T17:07:18.384237Z","iopub.status.idle":"2025-10-09T17:07:19.858189Z","shell.execute_reply.started":"2025-10-09T17:07:18.384222Z","shell.execute_reply":"2025-10-09T17:07:19.857523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define preprocessors\npreprocessors = {\n    \"CustomCNN\": None,\n    \"VGG16\": vgg16_preprocess,\n    \"MobileNetV2\": mobilenet_preprocess,\n    \"EfficientNetB0\": efficientnet_preprocess,\n    \"Ensemble\": efficientnet_preprocess  # Ensemble uses EfficientNetB0 preprocessing\n}\n\n# üî• Plot Grad-CAMs including Ensemble\nplt.figure(figsize=(20, 8))  # Adjusted figure size for 5 subplots\nfor i, (name, model) in enumerate(models.items()):\n    grad_cam_img = generate_grad_cam(model, sample_img, true_label, preprocessors[name])\n    plt.subplot(1, 5, i+1)\n    plt.imshow(grad_cam_img)\n    plt.title(f\"{name} Grad-CAM\")\n    plt.axis('off')\n\n# Add Ensemble Grad-CAM by averaging individual model contributions\nensemble_grads = np.mean([generate_grad_cam(model, sample_img, true_label, preprocessors[name])[:, :, ::-1] \n                         for name, model in models.items()], axis=0)  # Convert BGR to RGB\nplt.subplot(1, 5, 5)\nplt.imshow(ensemble_grads)\nplt.title(\"Ensemble Grad-CAM\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T17:07:19.859101Z","iopub.execute_input":"2025-10-09T17:07:19.859350Z","iopub.status.idle":"2025-10-09T17:07:22.391411Z","shell.execute_reply.started":"2025-10-09T17:07:19.859330Z","shell.execute_reply":"2025-10-09T17:07:22.390577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ‚úÖ Step 12: Accuracy and Loss Curves\nfor name, history in histories.items():\n    plt.figure(figsize=(10, 4))\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'], label='Train')\n    plt.plot(history.history['val_accuracy'], label='Validation')\n    plt.title(f'{name} Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='Train')\n    plt.plot(history.history['val_loss'], label='Validation')\n    plt.title(f'{name} Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T17:07:22.392356Z","iopub.execute_input":"2025-10-09T17:07:22.392620Z","iopub.status.idle":"2025-10-09T17:07:23.687661Z","shell.execute_reply.started":"2025-10-09T17:07:22.392600Z","shell.execute_reply":"2025-10-09T17:07:23.686759Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ‚úÖ Step 13: Compare Validation Accuracies Across Models\nplt.figure(figsize=(10, 6))\nfor name, history in histories.items():\n    plt.plot(history.history['val_accuracy'], label=f'{name}')\n\nplt.title('Validation Accuracy Comparison')\nplt.xlabel('Epochs')\nplt.ylabel('Validation Accuracy')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T17:07:23.688387Z","iopub.execute_input":"2025-10-09T17:07:23.688606Z","iopub.status.idle":"2025-10-09T17:07:23.879394Z","shell.execute_reply.started":"2025-10-09T17:07:23.688580Z","shell.execute_reply":"2025-10-09T17:07:23.878758Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ‚úÖ Step 12: Combined Accuracy and Loss Curves for All Models\n\n# Plot all model accuracies on one graph\nplt.figure(figsize=(10, 5))\nfor name, history in histories.items():\n    if 'val_accuracy' in history.history:\n        plt.plot(history.history['val_accuracy'], label=f'{name} (val)')\nplt.title('üìà Validation Accuracy Comparison')\nplt.xlabel('Epochs')\nplt.ylabel('Validation Accuracy')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot all model losses on one graph\nplt.figure(figsize=(10, 5))\nfor name, history in histories.items():\n    if 'val_loss' in history.history:\n        plt.plot(history.history['val_loss'], label=f'{name} (val)')\nplt.title('üìâ Validation Loss Comparison')\nplt.xlabel('Epochs')\nplt.ylabel('Validation Loss')\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T17:07:23.880321Z","iopub.execute_input":"2025-10-09T17:07:23.880577Z","iopub.status.idle":"2025-10-09T17:07:24.255414Z","shell.execute_reply.started":"2025-10-09T17:07:23.880553Z","shell.execute_reply":"2025-10-09T17:07:24.254605Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ‚úÖ Step 12: Combined Accuracy and Loss Curves for All Models\n\n# Create a figure with two subplots side by side\nplt.figure(figsize=(20, 5))\n\n# Plot all model validation accuracies on the left subplot\nplt.subplot(1, 2, 1)\nfor name, history in histories.items():\n    if 'val_accuracy' in history.history:\n        plt.plot(history.history['val_accuracy'], label=f'{name} (val)')\n# Add ensemble validation accuracy (using Step 8 ensemble_acc as a proxy, adjust if ensemble history exists)\nplt.axhline(y=ensemble_acc, color='r', linestyle='--', label='Ensemble (val)')\nplt.title('Validation Accuracy Comparison')\nplt.xlabel('Epochs')\nplt.ylabel('Validation Accuracy')\nplt.legend()\nplt.grid(True)\n\n# Plot all model validation losses on the right subplot\nplt.subplot(1, 2, 2)\nfor name, history in histories.items():\n    if 'val_loss' in history.history:\n        plt.plot(history.history['val_loss'], label=f'{name} (val)')\n# Add ensemble validation loss (placeholder; replace with ensemble history if available)\nplt.axhline(y=np.mean([history.history['val_loss'][-1] for history in histories.values() if 'val_loss' in history.history]), \n            color='r', linestyle='--', label='Ensemble (val approx)')\nplt.title('Validation Loss Comparison')\nplt.xlabel('Epochs')\nplt.ylabel('Validation Loss')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T17:07:24.256562Z","iopub.execute_input":"2025-10-09T17:07:24.256872Z","iopub.status.idle":"2025-10-09T17:07:24.689466Z","shell.execute_reply.started":"2025-10-09T17:07:24.256847Z","shell.execute_reply":"2025-10-09T17:07:24.688707Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ‚úÖ Step 12: Combined Training Accuracy and Loss for All Models\n\n# Plot all model training accuracies on one graph\nplt.figure(figsize=(10, 5))\nfor name, history in histories.items():\n    if 'accuracy' in history.history:\n        plt.plot(history.history['accuracy'], label=f'{name} (train)')\nplt.title('üìà Training Accuracy Comparison')\nplt.xlabel('Epochs')\nplt.ylabel('Training Accuracy')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot all model training losses on one graph\nplt.figure(figsize=(10, 5))\nfor name, history in histories.items():\n    if 'loss' in history.history:\n        plt.plot(history.history['loss'], label=f'{name} (train)')\nplt.title('üìâ Training Loss Comparison')\nplt.xlabel('Epochs')\nplt.ylabel('Training Loss')\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T17:07:24.690295Z","iopub.execute_input":"2025-10-09T17:07:24.690579Z","iopub.status.idle":"2025-10-09T17:07:25.045624Z","shell.execute_reply.started":"2025-10-09T17:07:24.690554Z","shell.execute_reply":"2025-10-09T17:07:25.044870Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ‚úÖ Step 12: Combined Training Accuracy and Loss for All Models\n\n# Plot all model training accuracies on one graph\nplt.figure(figsize=(10, 5))\nfor name, history in histories.items():\n    if 'accuracy' in history.history:\n        plt.plot(history.history['accuracy'], label=f'{name} (train)')\n# Add ensemble accuracy (using Step 8 ensemble_acc as a proxy, adjust if ensemble history exists)\nplt.axhline(y=ensemble_acc, color='r', linestyle='--', label='Ensemble (train)')\nplt.title('Model Accuracy Comparison')\nplt.xlabel('Epochs')\nplt.ylabel('Model Accuracy')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot all model training losses on one graph\nplt.figure(figsize=(10, 5))\nfor name, history in histories.items():\n    if 'loss' in history.history:\n        plt.plot(history.history['loss'], label=f'{name} (train)')\n# Add ensemble loss (placeholder; replace with ensemble history if available)\nplt.axhline(y=np.mean([history.history['loss'][-1] for history in histories.values() if 'loss' in history.history]), \n            color='r', linestyle='--', label='Ensemble (train approx)')\nplt.title('Model Loss Comparison')\nplt.xlabel('Epochs')\nplt.ylabel('Model Loss')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T17:07:25.048878Z","iopub.execute_input":"2025-10-09T17:07:25.049125Z","iopub.status.idle":"2025-10-09T17:07:25.424260Z","shell.execute_reply.started":"2025-10-09T17:07:25.049108Z","shell.execute_reply":"2025-10-09T17:07:25.423617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ‚úÖ Step 12: Combined Training Accuracy and Loss for All Models\n\n# Create a figure with two subplots side by side\nplt.figure(figsize=(20, 5))\n\n# Plot all model training accuracies on the left subplot\nplt.subplot(1, 2, 1)\nfor name, history in histories.items():\n    if 'accuracy' in history.history:\n        plt.plot(history.history['accuracy'], label=f'{name} (train)')\n# Add ensemble accuracy (using Step 8 ensemble_acc as a proxy, adjust if ensemble history exists)\nplt.axhline(y=ensemble_acc, color='r', linestyle='--', label='Ensemble (train)')\nplt.title('Model Accuracy Comparison')\nplt.xlabel('Epochs')\nplt.ylabel('Model Accuracy')\nplt.legend()\nplt.grid(True)\n\n# Plot all model training losses on the right subplot\nplt.subplot(1, 2, 2)\nfor name, history in histories.items():\n    if 'loss' in history.history:\n        plt.plot(history.history['loss'], label=f'{name} (train)')\n# Add ensemble loss (placeholder; replace with ensemble history if available)\nplt.axhline(y=np.mean([history.history['loss'][-1] for history in histories.values() if 'loss' in history.history]), \n            color='r', linestyle='--', label='Ensemble (train approx)')\nplt.title('Model Loss Comparison')\nplt.xlabel('Epochs')\nplt.ylabel('Model Loss')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T17:07:25.425192Z","iopub.execute_input":"2025-10-09T17:07:25.425461Z","iopub.status.idle":"2025-10-09T17:07:25.846646Z","shell.execute_reply.started":"2025-10-09T17:07:25.425438Z","shell.execute_reply":"2025-10-09T17:07:25.845908Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ‚úÖ Step 14: Accuracy/Loss Curves (Train vs Validation)\nfor name, history in histories.items():\n    acc = history.history.get('accuracy', [])\n    val_acc = history.history.get('val_accuracy', [])\n    loss = history.history.get('loss', [])\n    val_loss = history.history.get('val_loss', [])\n\n    if acc and val_acc:\n        plt.figure(figsize=(12, 4))\n\n        # Accuracy plot\n        plt.subplot(1, 2, 1)\n        plt.plot(acc, label='Train Accuracy')\n        plt.plot(val_acc, label='Val Accuracy')\n        plt.title(f'{name} - Accuracy Curve')\n        plt.xlabel('Epochs')\n        plt.ylabel('Accuracy')\n        plt.legend()\n        plt.grid(True)\n\n        # Loss plot\n        plt.subplot(1, 2, 2)\n        plt.plot(loss, label='Train Loss')\n        plt.plot(val_loss, label='Val Loss')\n        plt.title(f'{name} - Loss Curve')\n        plt.xlabel('Epochs')\n        plt.ylabel('Loss')\n        plt.legend()\n        plt.grid(True)\n\n        plt.tight_layout()\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T17:07:25.847504Z","iopub.execute_input":"2025-10-09T17:07:25.847733Z","iopub.status.idle":"2025-10-09T17:07:27.192561Z","shell.execute_reply.started":"2025-10-09T17:07:25.847716Z","shell.execute_reply":"2025-10-09T17:07:27.191752Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score\n\n# ‚úÖ Step 15: Model Performance Comparison Table\nmetrics_table = []\n\nfor name, preds in model_preds.items():\n    y_pred_labels = np.argmax(preds, axis=1)\n    acc = accuracy_score(y_val, y_pred_labels)\n    prec = precision_score(y_val, y_pred_labels, average='macro')\n    rec = recall_score(y_val, y_pred_labels, average='macro')\n    f1 = f1_score(y_val, y_pred_labels, average='macro')\n    metrics_table.append([name, acc, prec, rec, f1])\n\n# Add Ensemble\nensemble_preds = np.argmax(np.mean(list(model_preds.values()), axis=0), axis=1)\nensemble_acc = accuracy_score(y_val, ensemble_preds)\nensemble_prec = precision_score(y_val, ensemble_preds, average='macro')\nensemble_rec = recall_score(y_val, ensemble_preds, average='macro')\nensemble_f1 = f1_score(y_val, ensemble_preds, average='macro')\nmetrics_table.append([\"Ensemble\", ensemble_acc, ensemble_prec, ensemble_rec, ensemble_f1])\n\n# Create DataFrame\ndf_metrics = pd.DataFrame(metrics_table, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"])\ndf_metrics = df_metrics.sort_values(\"Accuracy\", ascending=False)\nprint(df_metrics)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T17:07:27.193286Z","iopub.execute_input":"2025-10-09T17:07:27.193505Z","iopub.status.idle":"2025-10-09T17:07:27.226252Z","shell.execute_reply.started":"2025-10-09T17:07:27.193481Z","shell.execute_reply":"2025-10-09T17:07:27.225643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Optional: Visual Comparison\ndf_metrics.set_index(\"Model\")[[\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]].plot(kind=\"bar\", figsize=(12, 6))\nplt.title(\"üìä Model Performance Comparison\")\nplt.ylabel(\"Score\")\nplt.ylim(0, 1.05)\nplt.grid(True)\nplt.xticks(rotation=45)\nplt.legend(loc='lower right')\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T17:07:27.227013Z","iopub.execute_input":"2025-10-09T17:07:27.227377Z","iopub.status.idle":"2025-10-09T17:07:27.489675Z","shell.execute_reply.started":"2025-10-09T17:07:27.227346Z","shell.execute_reply":"2025-10-09T17:07:27.488892Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow.keras.backend as K\nimport tensorflow as tf\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Grad-CAM Function\ndef generate_grad_cam(model, image, label_index, preprocess=None):\n    img_tensor = np.expand_dims(image, axis=0)\n    if preprocess:\n        img_tensor = preprocess(img_tensor.copy())\n    else:\n        img_tensor = img_tensor / 255.0\n\n    # Detect last Conv layer\n    last_conv_layer = None\n    for layer in reversed(model.layers):\n        if isinstance(layer, tf.keras.layers.Conv2D):\n            last_conv_layer = layer.name\n            break\n\n    grad_model = Model([model.inputs], [model.get_layer(last_conv_layer).output, model.output])\n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(img_tensor)\n        loss = predictions[:, label_index]\n\n    grads = tape.gradient(loss, conv_outputs)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n    conv_outputs = conv_outputs[0]\n    heatmap = tf.reduce_mean(tf.multiply(pooled_grads, conv_outputs), axis=-1)\n    heatmap = np.maximum(heatmap, 0) / tf.math.reduce_max(heatmap + tf.keras.backend.epsilon())\n    heatmap = cv2.resize(heatmap.numpy(), (224, 224))\n    return heatmap  # Return raw heatmap for averaging\n\n# Function to overlay heatmap on image\ndef overlay_heatmap(heatmap, image):\n    heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n    superimposed_img = cv2.addWeighted(image.astype(np.uint8), 0.6, heatmap_colored, 0.4, 0)\n    return superimposed_img\n\n# Define preprocessors\npreprocessors = {\n    \"CustomCNN\": None,\n    \"VGG16\": vgg16_preprocess,\n    \"MobileNetV2\": mobilenet_preprocess,\n    \"EfficientNetB0\": efficientnet_preprocess,\n    \"Ensemble\": efficientnet_preprocess  # Using EfficientNetB0 preprocessing for consistency\n}\n\n# Sample image from validation set\nsample_img = X_val_resized[0]\ntrue_label = y_val[0]\n\n# Generate Grad-CAM heatmaps for all models\nheatmaps = {}\nfor name, model in models.items():\n    print(f\"üîç Generating Grad-CAM heatmap for {name}...\")\n    heatmap = generate_grad_cam(model, sample_img, true_label, preprocess=preprocessors[name])\n    heatmaps[name] = heatmap\n\n# Compute ensemble heatmap by averaging individual heatmaps\nensemble_heatmap = np.mean(list(heatmaps.values()), axis=0)\nensemble_heatmap = np.maximum(ensemble_heatmap, 0) / np.max(ensemble_heatmap)  # Normalize\n\n# Overlay heatmaps on the original image\nplt.figure(figsize=(20, 8))\nfor i, (name, model) in enumerate(models.items()):\n    grad_cam_img = overlay_heatmap(heatmaps[name], sample_img)\n    plt.subplot(1, 5, i + 1)\n    plt.imshow(grad_cam_img)\n    plt.title(f\"{name} Grad-CAM\")\n    plt.axis('off')\n\n# Overlay ensemble heatmap\nensemble_grad_cam_img = overlay_heatmap(ensemble_heatmap, sample_img)\nplt.subplot(1, 5, 5)\nplt.imshow(ensemble_grad_cam_img)\nplt.title(\"Ensemble Grad-CAM\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T17:09:36.116834Z","iopub.execute_input":"2025-10-09T17:09:36.117641Z","iopub.status.idle":"2025-10-09T17:09:37.789745Z","shell.execute_reply.started":"2025-10-09T17:09:36.117614Z","shell.execute_reply":"2025-10-09T17:09:37.788827Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow.keras.backend as K\nimport tensorflow as tf\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Grad-CAM Function\ndef generate_grad_cam(model, image, label_index, preprocess=None):\n    img_tensor = np.expand_dims(image, axis=0)\n    if preprocess:\n        img_tensor = preprocess(img_tensor.copy())\n    else:\n        img_tensor = img_tensor / 255.0\n\n    # Detect last Conv layer\n    last_conv_layer = None\n    for layer in reversed(model.layers):\n        if isinstance(layer, tf.keras.layers.Conv2D):\n            last_conv_layer = layer.name\n            break\n\n    grad_model = Model([model.inputs], [model.get_layer(last_conv_layer).output, model.output])\n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(img_tensor)\n        loss = predictions[:, label_index]\n\n    grads = tape.gradient(loss, conv_outputs)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n    conv_outputs = conv_outputs[0]\n    heatmap = tf.reduce_mean(tf.multiply(pooled_grads, conv_outputs), axis=-1)\n    heatmap = np.maximum(heatmap, 0) / tf.math.reduce_max(heatmap + tf.keras.backend.epsilon())\n    heatmap = cv2.resize(heatmap.numpy(), (224, 224))\n    return heatmap\n\n# Function to overlay heatmap on image\ndef overlay_heatmap(heatmap, image):\n    heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n    superimposed_img = cv2.addWeighted(image.astype(np.uint8), 0.6, heatmap_colored, 0.4, 0)\n    return superimposed_img\n\n# Define preprocessors\npreprocessors = {\n    \"CustomCNN\": None,\n    \"VGG16\": vgg16_preprocess,\n    \"MobileNetV2\": mobilenet_preprocess,\n    \"EfficientNetB0\": efficientnet_preprocess,\n    \"Ensemble\": efficientnet_preprocess  # Using EfficientNetB0 preprocessing for consistency\n}\n\n# Sample image from validation set\nsample_img = X_val_resized[0]\n\n# Get predictions for the sample image\nheatmaps = {}\npredictions = {}\nfor name, model in models.items():\n    img_tensor = np.expand_dims(sample_img, axis=0)\n    if preprocessors[name]:\n        img_tensor = preprocessors[name](img_tensor.copy())\n    else:\n        img_tensor = img_tensor / 255.0\n    pred = model.predict(img_tensor)\n    predicted_label = np.argmax(pred)\n    predictions[name] = predicted_label\n    heatmap = generate_grad_cam(model, sample_img, predicted_label, preprocess=preprocessors[name])\n    heatmaps[name] = heatmap\n\n# Compute ensemble prediction and heatmap\nensemble_preds = np.mean([models[name].predict(np.expand_dims(sample_img, axis=0) if not preprocessors[name] else preprocessors[name](np.expand_dims(sample_img, axis=0))) for name in models], axis=0)\nensemble_label = np.argmax(ensemble_preds)\nensemble_heatmap = np.mean(list(heatmaps.values()), axis=0)\nensemble_heatmap = np.maximum(ensemble_heatmap, 0) / np.max(ensemble_heatmap)\n\n# Overlay heatmaps on the original image\nplt.figure(figsize=(20, 8))\nfor i, (name, model) in enumerate(models.items()):\n    grad_cam_img = overlay_heatmap(heatmaps[name], sample_img)\n    plt.subplot(1, 5, i + 1)\n    plt.imshow(grad_cam_img)\n    plt.title(f\"{name} Grad-CAM \")\n    plt.axis('off')\n\n# Overlay ensemble heatmap\nensemble_grad_cam_img = overlay_heatmap(ensemble_heatmap, sample_img)\nplt.subplot(1, 5, 5)\nplt.imshow(ensemble_grad_cam_img)\nplt.title(f\"Ensemble Grad-CAM\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T17:14:20.118501Z","iopub.execute_input":"2025-10-09T17:14:20.119136Z","iopub.status.idle":"2025-10-09T17:14:22.466226Z","shell.execute_reply.started":"2025-10-09T17:14:20.119110Z","shell.execute_reply":"2025-10-09T17:14:22.465394Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow.keras.backend as K\nimport tensorflow as tf\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Guided Backpropagation Function\ndef guided_backprop(model, image, label_index, preprocess=None):\n    img_tensor = np.expand_dims(image, axis=0).astype(np.float32)\n    if preprocess:\n        img_tensor = preprocess(img_tensor.copy())\n    else:\n        img_tensor = img_tensor / 255.0\n\n    img_tensor = tf.convert_to_tensor(img_tensor, dtype=tf.float32)\n\n    @tf.custom_gradient\n    def guided_relu(x):\n        def grad(dy):\n            return tf.cast(dy > 0, \"float32\") * tf.cast(x > 0, \"float32\") * dy\n        return tf.nn.relu(x), grad\n\n    layer_dict = [layer for layer in model.layers if hasattr(layer, 'activation')]\n    for layer in layer_dict:\n        if layer.activation == tf.keras.activations.relu:\n            layer.activation = guided_relu\n\n    with tf.GradientTape() as tape:\n        tape.watch(img_tensor)\n        preds = model(img_tensor, training=False)\n        loss = preds[:, label_index]\n\n    grads = tape.gradient(loss, img_tensor)[0]\n    guided_grads = tf.cast(grads > 0, \"float32\") * tf.cast(img_tensor > 0, \"float32\") * grads\n    guided_grads = tf.reduce_mean(guided_grads, axis=-1)  # Ensure single channel\n    guided_grads = tf.squeeze(guided_grads)  # Remove any extra dimensions\n    guided_grads = cv2.resize(guided_grads.numpy(), (224, 224))\n    return guided_grads\n\n# Grad-CAM Function\ndef generate_grad_cam(model, image, label_index, preprocess=None):\n    img_tensor = np.expand_dims(image, axis=0).astype(np.float32)\n    if preprocess:\n        img_tensor = preprocess(img_tensor.copy())\n    else:\n        img_tensor = img_tensor / 255.0\n\n    last_conv_layer = None\n    for layer in reversed(model.layers):\n        if isinstance(layer, tf.keras.layers.Conv2D):\n            last_conv_layer = layer.name\n            break\n\n    grad_model = Model([model.inputs], [model.get_layer(last_conv_layer).output, model.output])\n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(img_tensor)\n        loss = predictions[:, label_index]\n\n    grads = tape.gradient(loss, conv_outputs)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n    conv_outputs = conv_outputs[0]\n    heatmap = tf.reduce_mean(tf.multiply(pooled_grads, conv_outputs), axis=-1)\n    heatmap = np.maximum(heatmap, 0) / tf.math.reduce_max(heatmap + tf.keras.backend.epsilon())\n    heatmap = cv2.resize(heatmap.numpy(), (224, 224))\n    return heatmap\n\n# Function to overlay heatmap on image\ndef overlay_heatmap(heatmap, image, guided_grads=None):\n    heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n    if guided_grads is not None:\n        # Ensure guided_grads is 2D and of type uint8\n        if len(guided_grads.shape) > 2:\n            guided_grads = guided_grads[..., 0]  # Take the first channel if multi-channel\n        guided_grads_normalized = cv2.normalize(guided_grads, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n        guided_grads_colored = cv2.applyColorMap(guided_grads_normalized, cv2.COLORMAP_JET)\n        heatmap_colored = cv2.addWeighted(heatmap_colored, 0.7, guided_grads_colored, 0.3, 0)\n    superimposed_img = cv2.addWeighted(image.astype(np.uint8), 0.6, heatmap_colored, 0.4, 0)\n    return superimposed_img\n\n# Define preprocessors\npreprocessors = {\n    \"CustomCNN\": None,\n    \"VGG16\": vgg16_preprocess,\n    \"MobileNetV2\": mobilenet_preprocess,\n    \"EfficientNetB0\": efficientnet_preprocess,\n    \"Ensemble\": efficientnet_preprocess\n}\n\n# Sample image from validation set\nsample_img = X_val_resized[0]\n\n# Get predictions and heatmaps\nheatmaps = {}\nguided_grads = {}\npredictions = {}\nfor name, model in models.items():\n    img_tensor = np.expand_dims(sample_img, axis=0).astype(np.float32)\n    if preprocessors[name]:\n        img_tensor = preprocessors[name](img_tensor.copy())\n    else:\n        img_tensor = img_tensor / 255.0\n    pred = model.predict(img_tensor)\n    predicted_label = np.argmax(pred)\n    predictions[name] = predicted_label\n    heatmap = generate_grad_cam(model, sample_img, predicted_label, preprocess=preprocessors[name])\n    guided_grad = guided_backprop(model, sample_img, predicted_label, preprocess=preprocessors[name])\n    heatmaps[name] = heatmap\n    guided_grads[name] = guided_grad\n\n# Compute ensemble prediction and weighted heatmap\nensemble_preds = np.mean([models[name].predict(np.expand_dims(sample_img, axis=0).astype(np.float32) if not preprocessors[name] else preprocessors[name](np.expand_dims(sample_img, axis=0).astype(np.float32))) for name in models], axis=0)\nensemble_label = np.argmax(ensemble_preds)\nconfidences = [pred.max() for pred in [models[name].predict(np.expand_dims(sample_img, axis=0).astype(np.float32) if not preprocessors[name] else preprocessors[name](np.expand_dims(sample_img, axis=0).astype(np.float32))) for name in models]]\nensemble_heatmap = np.average(list(heatmaps.values()), weights=confidences, axis=0)\nensemble_heatmap = np.maximum(ensemble_heatmap, 0) / np.max(ensemble_heatmap)\nensemble_guided_grad = np.average(list(guided_grads.values()), weights=confidences, axis=0)\n\n# Overlay heatmaps\nplt.figure(figsize=(20, 8))\nfor i, (name, model) in enumerate(models.items()):\n    grad_cam_img = overlay_heatmap(heatmaps[name], sample_img, guided_grads[name])\n    plt.subplot(1, 5, i + 1)\n    plt.imshow(grad_cam_img)\n    plt.title(f\"{name} Grad-CAM (Pred: {predictions[name]})\")\n    plt.axis('off')\n\n# Overlay ensemble heatmap\nensemble_grad_cam_img = overlay_heatmap(ensemble_heatmap, sample_img, ensemble_guided_grad)\nplt.subplot(1, 5, 5)\nplt.imshow(ensemble_grad_cam_img)\nplt.title(f\"Ensemble Grad-CAM (Pred: {ensemble_label})\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T17:23:56.801446Z","iopub.execute_input":"2025-10-09T17:23:56.802243Z","iopub.status.idle":"2025-10-09T17:24:00.297135Z","shell.execute_reply.started":"2025-10-09T17:23:56.802218Z","shell.execute_reply":"2025-10-09T17:24:00.296377Z"}},"outputs":[],"execution_count":null}]}